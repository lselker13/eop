{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import itertools\n",
    "import subprocess\n",
    "import os\n",
    "from glob import glob\n",
    "\n",
    "\n",
    "\n",
    "from opt_targeted_transfers import standardize\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "from opt_targeted_transfers import Dataset, split\n",
    "def get_row_from_metadata(metadata, covariate_name):\n",
    "    \"\"\"\n",
    "    Extracts a specific row from the metadata DataFrame based on the covariate name.\n",
    "\n",
    "    :param metadata: DataFrame containing metadata.\n",
    "    :param covariate_name: Name of the covariate to extract.\n",
    "    :return: Row corresponding to the specified covariate name.\n",
    "    \"\"\"\n",
    "    return metadata.loc[metadata['variable_name'] == covariate_name].squeeze()\n",
    "\n",
    "def all_rows_from_metadata_containing(metadata, substring):\n",
    "    \"\"\"\n",
    "    Extracts all rows from the metadata DataFrame that contain a specific substring in the variable name.\n",
    "\n",
    "    :param metadata: DataFrame containing metadata.\n",
    "    :param substring: Substring to search for in the variable names.\n",
    "    :return: DataFrame containing all rows with variable names that contain the substring.\n",
    "    \"\"\"\n",
    "    return metadata[metadata['variable_name'].str.contains(substring, na=False)].reset_index(drop=True)\n",
    "\n",
    "def all_column_names_containing(df, substring):\n",
    "    \"\"\"\n",
    "    Extracts all column names from the DataFrame that contain a specific substring.\n",
    "\n",
    "    :param df: DataFrame to search for column names.\n",
    "    :param substring: Substring to search for in the column names.\n",
    "    :return: List of column names containing the specified substring.\n",
    "    \"\"\"\n",
    "    return [col for col in df.columns if substring in col]\n",
    "\n",
    "\n",
    "def find_equivalent_columns(data, summary, numeric_tolerance=1e-6, categorical_threshold=0.99):\n",
    "    \"\"\"\n",
    "    Find pairs of columns in a DataFrame that are informationally equivalent.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    data : pandas DataFrame\n",
    "        The DataFrame to analyze\n",
    "    numeric_tolerance : float, default 1e-6\n",
    "        Tolerance for considering numeric columns equal or proportional\n",
    "    categorical_threshold : float, default 0.99\n",
    "        Threshold for considering categorical columns equivalent (percentage match)\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    list of tuples\n",
    "        Each tuple contains (col1, col2, relationship_type)\n",
    "        where relationship_type is one of: 'identical', 'proportional', 'categorical_equivalent'\n",
    "    \"\"\"\n",
    "    equivalent_pairs = []\n",
    "    columns = data.columns\n",
    "    \n",
    "    # Get column types\n",
    "    \n",
    "    numeric_cols = summary[summary.data_type == 'numeric'].variable_name.tolist()\n",
    "    categorical_cols = summary[summary.data_type == 'categorical'].variable_name.tolist()\n",
    "\n",
    "    # Identify constant columns\n",
    "    constant_cols = []\n",
    "    for col in columns:\n",
    "        unique_values = data[col].dropna().unique()\n",
    "        if len(unique_values) <= 1:\n",
    "            constant_cols.append(col)\n",
    "\n",
    "    # Print constant columns if verbose\n",
    "    if len(constant_cols) > 0:\n",
    "        print(\"Constant columns:\")\n",
    "        for col in constant_cols:\n",
    "            print(col)\n",
    "        print()\n",
    "    \n",
    "\n",
    "    # Remove constant columns from numeric and categorical lists\n",
    "    numeric_cols = [col for col in numeric_cols if col not in constant_cols]\n",
    "    categorical_cols = [col for col in categorical_cols if col not in constant_cols]\n",
    "\n",
    "    # remove missingness-indicator columns\n",
    "    missingness_cols = list(\n",
    "        set(all_column_names_containing(data, '_missing') + \n",
    "        all_column_names_containing(data, '_m'))\n",
    "    )\n",
    "    numeric_cols = [col for col in numeric_cols if col not in missingness_cols]\n",
    "    categorical_cols = [col for col in categorical_cols if col not in missingness_cols]\n",
    "    \n",
    "    # Check numeric columns for equality or proportionality\n",
    "    for col1, col2 in itertools.combinations(numeric_cols, 2):\n",
    "\n",
    "        # Check for identical values first\n",
    "        if data[col1].equals(data[col2]):\n",
    "            equivalent_pairs.append((col1, col2, 'identical'))\n",
    "            continue\n",
    "            \n",
    "        # Check for identical values where neither is zero\n",
    "        valid_mask = ~data[col1].isna() & ~data[col2].isna()\n",
    "        if np.allclose(data.loc[valid_mask, col1], data.loc[valid_mask, col2], \n",
    "                      rtol=numeric_tolerance, atol=numeric_tolerance):\n",
    "            equivalent_pairs.append((col1, col2, 'nearly_identical'))\n",
    "            continue\n",
    "        \n",
    "        # For rows with zeros, check if the columns are exactly equal\n",
    "        zero_mask = (data[col1] == 0) | (data[col2] == 0)\n",
    "        non_zero_mask = ~zero_mask & valid_mask\n",
    "        \n",
    "        # Check if the columns have the same values where zeros are present\n",
    "        if zero_mask.any():\n",
    "            zero_equality = (data.loc[zero_mask & valid_mask, col1] == \n",
    "                             data.loc[zero_mask & valid_mask, col2]).all()\n",
    "        else:\n",
    "            zero_equality = True\n",
    "            \n",
    "        # Check for proportional relationship in non-zero values\n",
    "        if non_zero_mask.sum() > 10:  # Require at least some non-zero values\n",
    "            ratios = data.loc[non_zero_mask, col2].astype(int) / data.loc[non_zero_mask, col1].astype(int)\n",
    "            ratio_std = ratios.std()\n",
    "            \n",
    "            # If standard deviation of ratios is very small, columns are proportional\n",
    "            if ratio_std < numeric_tolerance and zero_equality:\n",
    "                ratio = ratios.mean()\n",
    "                equivalent_pairs.append((col1, col2, f'proportional (factor: {ratio:.4f})'))\n",
    "    \n",
    "    # Create a list of all columns to check for categorical equivalence\n",
    "    # This includes both explicit categorical columns and numeric columns\n",
    "    all_potential_categorical_cols = categorical_cols + numeric_cols\n",
    "    \n",
    "    # Check all columns for equivalent categorical mappings\n",
    "    for col1, col2 in itertools.combinations(all_potential_categorical_cols, 2):\n",
    "        # Skip if identical columns or already identified as identical or proportional\n",
    "        if col1 == col2 or any((col1, col2, rel) in equivalent_pairs for rel in \n",
    "                               ['identical', 'nearly_identical', 'proportional']):\n",
    "            continue\n",
    "            \n",
    "        # Get unique values for both columns\n",
    "        unique_vals1 = data[col1].dropna().unique()\n",
    "        unique_vals2 = data[col2].dropna().unique()\n",
    "        \n",
    "        # Skip if columns have different number of unique values\n",
    "        if len(unique_vals1) != len(unique_vals2):\n",
    "            continue\n",
    "            \n",
    "        # Skip if too many unique values (likely not categorical)\n",
    "        if len(unique_vals1) > 100:  # Arbitrary threshold, adjust as needed\n",
    "            continue\n",
    "            \n",
    "        # Create a mapping table between values in both columns\n",
    "        mapping_df = data[[col1, col2]].dropna().drop_duplicates()\n",
    "        \n",
    "        # Check if mapping is one-to-one (each value in col1 maps to exactly one value in col2)\n",
    "        is_one_to_one = True\n",
    "        \n",
    "        # Check col1 -> col2 mapping\n",
    "        for val in unique_vals1:\n",
    "            corresponding_vals = data.loc[data[col1] == val, col2].dropna().unique()\n",
    "            if len(corresponding_vals) != 1:\n",
    "                is_one_to_one = False\n",
    "                break\n",
    "                \n",
    "        # Check col2 -> col1 mapping\n",
    "        if is_one_to_one:\n",
    "            for val in unique_vals2:\n",
    "                corresponding_vals = data.loc[data[col2] == val, col1].dropna().unique()\n",
    "                if len(corresponding_vals) != 1:\n",
    "                    is_one_to_one = False\n",
    "                    break\n",
    "        \n",
    "        if is_one_to_one:\n",
    "            # If we create a new column using the mapping, it should match the original\n",
    "            val_mapping = dict(zip(mapping_df[col1], mapping_df[col2]))\n",
    "            \n",
    "            # Apply mapping and handle NaN values\n",
    "            mapped_values = data[col1].map(val_mapping)\n",
    "            \n",
    "            # Count matches (ignoring NaN values)\n",
    "            valid_mask = ~data[col1].isna() & ~data[col2].isna()\n",
    "            if valid_mask.sum() > 0:\n",
    "                match_percentage = (mapped_values == data[col2])[valid_mask].mean()\n",
    "                \n",
    "                if match_percentage >= categorical_threshold:\n",
    "                    # Determine if both are numeric or mixed types\n",
    "                    if col1 in numeric_cols and col2 in numeric_cols:\n",
    "                        relationship = 'numeric_categorical_equivalent'\n",
    "                    else:\n",
    "                        relationship = 'categorical_equivalent'\n",
    "                    equivalent_pairs.append((col1, col2, relationship))\n",
    "    \n",
    "    return equivalent_pairs\n",
    "\n",
    "\n",
    "\n",
    "def get_data_for_geo_extrapolation(data, summary, geo_extrapolation):\n",
    "    \"\"\"\n",
    "    Preprocess the testing data for geo-extrapolation.\n",
    "\n",
    "    Args:\n",
    "        data (pd.DataFrame): The input data.\n",
    "        summary (pd.DataFrame): The summary data.\n",
    "    Returns:\n",
    "        pd.DataFrame: The preprocessed data without geographic identifiers\n",
    "    \"\"\"\n",
    "\n",
    "    geo_cols = summary[summary[\"geographic_indicator\"] == True][\n",
    "        \"variable_name\"\n",
    "    ].tolist()\n",
    "\n",
    "    coarse_geo_cols = summary[summary[\"geographic_indicator_coarser\"] == True][\n",
    "        \"variable_name\"\n",
    "    ].tolist()\n",
    "\n",
    "    remove_for_coarse = set(geo_cols) - set(coarse_geo_cols)\n",
    "    remove_for_coarse = list(remove_for_coarse)\n",
    "\n",
    "    if geo_extrapolation:\n",
    "        data = data.drop(columns=remove_for_coarse)\n",
    "    else:\n",
    "        1/0\n",
    "    return data\n",
    "\n",
    "\n",
    "def load_datasets(\n",
    "    trainpath, testpath, summarypath, geo_extrapolation, outcome='consumption_per_day_per_capita', \n",
    "    weight='headcount_adjusted_hh_wgt'\n",
    "):\n",
    "    \"\"\"\n",
    "    Load datasets.\n",
    "\n",
    "    Args:\n",
    "        trainpath (str): Path to the training data file.\n",
    "        testpath (str): Path to the test data file.\n",
    "        outcome (str): Outcome variable.\n",
    "        weight (str): Weight variable.\n",
    "\n",
    "    Returns:\n",
    "        train_dataset (Dataset): Training dataset.\n",
    "        test_dataset (Dataset): Test dataset.\n",
    "    \"\"\"\n",
    "    data1 = _load_data(trainpath)\n",
    "    data2 = _load_data(testpath)\n",
    "    summary = pd.read_parquet(summarypath)\n",
    "\n",
    "    data1 = get_data_for_geo_extrapolation(data1, summary, geo_extrapolation)\n",
    "    data2 = get_data_for_geo_extrapolation(data2, summary, geo_extrapolation)\n",
    "\n",
    "    all_data = pd.concat([data1, data2], ignore_index=True)\n",
    "    all_data = convert_to_onehot(all_data, summary)\n",
    "\n",
    "    train_data = _load_data(trainpath)\n",
    "    test_data = _load_data(testpath)\n",
    "\n",
    "    train_data = get_data_for_geo_extrapolation(train_data, summary, geo_extrapolation)\n",
    "    test_data = get_data_for_geo_extrapolation(test_data, summary, geo_extrapolation)\n",
    "    covs = list(train_data.columns)\n",
    "    covs.remove(outcome)\n",
    "    covs.remove(weight)\n",
    "\n",
    "    train_data = convert_to_onehot(train_data, summary)\n",
    "    test_data = convert_to_onehot(test_data, summary)\n",
    "\n",
    "    train_missing_columns = set(all_data.columns) - set(train_data.columns)\n",
    "    res = [train_data]\n",
    "    for col in train_missing_columns:\n",
    "        res.append(pd.DataFrame({col: np.zeros(len(train_data))}))\n",
    "    final_train_data = pd.concat(res, axis=1)\n",
    "\n",
    "    test_missing_columns = set(all_data.columns) - set(test_data.columns)\n",
    "    res = [test_data]\n",
    "    for col in test_missing_columns:\n",
    "        res.append(pd.DataFrame({col: np.zeros(len(test_data))}))\n",
    "    final_test_data = pd.concat(res, axis=1)\n",
    "\n",
    "    train_dataset = Dataset(\n",
    "        final_train_data.astype(\"float32\"), outcome=outcome, covs=covs, weight=weight\n",
    "    )\n",
    "    test_dataset = Dataset(\n",
    "        final_test_data.astype(\"float32\"), outcome=outcome, covs=covs, weight=weight\n",
    "    )\n",
    "    test_covariate_dataset = Dataset(\n",
    "        final_test_data.astype(\"float32\"), outcome=None, covs=covs, weight=weight\n",
    "    )\n",
    "\n",
    "    train_dataset, validation_dataset = split(train_dataset)\n",
    "\n",
    "    return train_dataset, validation_dataset, test_covariate_dataset, test_dataset\n",
    "\n",
    "\n",
    "def convert_to_onehot(df, summary):\n",
    "    \"\"\"\n",
    "    Convert categorical columns to one-hot encoding.\n",
    "\n",
    "    :param df: The input data.\n",
    "    :type df: pandas.DataFrame\n",
    "    :return new_df: The input data with one-hot encoding.\n",
    "    :rtype: pandas.DataFrame\n",
    "    \"\"\"\n",
    "    if \"type\" in summary.columns:\n",
    "        data_type = \"type\"\n",
    "    elif \"data_type\" in summary.columns:\n",
    "        data_type = \"data_type\"\n",
    "    if \"covariate\" in summary.columns:\n",
    "        covariate = \"covariate\"\n",
    "    elif \"variable_name\" in summary.columns:\n",
    "        covariate = \"variable_name\"\n",
    "\n",
    "    categorical_columns = summary[summary[data_type] == \"categorical\"][\n",
    "        covariate\n",
    "    ].tolist()\n",
    "\n",
    "\n",
    "    categorical_columns = [col for col in categorical_columns if col in df.columns]\n",
    "    # print top 5 categorical columns by number of distinct values\n",
    "    counts = {col: df[col].nunique(dropna=True) for col in categorical_columns}\n",
    "    if counts:\n",
    "        top5 = pd.Series(counts).sort_values(ascending=False).head(5)\n",
    "        print(\"Top 5 categorical columns by distinct values:\")\n",
    "        for col, cnt in top5.items():\n",
    "            print(f\"{col}: {cnt}\")\n",
    "    else:\n",
    "        print(\"No categorical columns found.\")\n",
    "    one_hot = pd.get_dummies(df[categorical_columns]).astype(np.float32)\n",
    "    df.drop(columns=categorical_columns, inplace=True)\n",
    "    new_df = pd.concat([df, one_hot], axis=1)\n",
    "    return new_df\n",
    "\n",
    "\n",
    "def _load_data(path):\n",
    "    \"\"\"\n",
    "    Load data.\n",
    "\n",
    "    Args:\n",
    "        path (str): Path to the data file.\n",
    "\n",
    "    Returns:\n",
    "        data_for_wgan (pd.DataFrame): Data for WGAN training.\n",
    "        data_wrapper (wgan.DataWrapper): DataWrapper object for WGAN training.\n",
    "    \"\"\"\n",
    "    data = pd.read_parquet(path)\n",
    "\n",
    "    if \"hhid\" in data.columns:\n",
    "        data = data.drop(columns=[\"hhid\"])\n",
    "    if \"case_id\" in data.columns:\n",
    "        data = data.drop(columns=[\"case_id\"])\n",
    "    if \"hh_id\" in data.columns:\n",
    "        data = data.drop(columns=[\"hh_id\"])\n",
    "    if \"hh_wgt\" in data.columns:\n",
    "        data = data.drop(columns=[\"hh_wgt\"])\n",
    "\n",
    "    return data.reset_index(drop=True)\n",
    "\n",
    "\"\"\"\n",
    "Done in this notebook\n",
    "- Ensure that missingness-indicator columns exist.\n",
    "    - You probably can't conclusively check that all are included, because \n",
    "       the data you get will not necessarily reveal which columns had missingness, but check \n",
    "       that there are some missingness columns, and none for categorical data.\n",
    "- Ensure there are no NaNs in the data.\n",
    "- Ensure column names:\n",
    "    - In data: \"hhid\" (if household ID is included), \"consumption_per_capita_per_day\", \"hh_wgt\".\n",
    "    - Consumption: Check mean and std for sanity. In a poor country, the mean should be low-mid single \n",
    "      digits: e.g., in Uganda, the mean is $3.80/day.\n",
    "- Check for columns that indicate units:\n",
    "    - If they are present, the corresponding numeric field should be standardized, e.g., all area units \n",
    "      adjusted to square meters.\n",
    "- Check that metadata and the dataset itself match:\n",
    "    - Every column in data is described in metadata and vice versa. It's also OK if `hhid` is not in the data at all.\n",
    "- In metadata:\n",
    "    - \"variable_name\".\n",
    "    - \"data_type\", with permitted values \"numeric\" and \"categorical\".\n",
    "    - \"geographic_indicator\".\n",
    "- Scan datatypes:\n",
    "    - In particular, make sure nothing is numeric which should be categorical.\n",
    "    - Ensure categorical-type columns have the appropriate type even if the categories are encoded as integers (if a \n",
    "      column is binary, with no missing values, it can be numeric or categorical).\n",
    "    - IDs of all kinds are strings even if they appear numeric.\n",
    "- Check for duplication\n",
    "- Check feasibility of stratification\n",
    "\"\"\";\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read in: india\n",
      "nullity: \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "hhid      0.0\n",
       "sector    0.0\n",
       "dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "empty string\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "latrine_type     0.100070\n",
       "roof_material    0.000057\n",
       "dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples:\n",
      "261746\n"
     ]
    }
   ],
   "source": [
    "\n",
    "data_path = Path('/data/eop/country_data')\n",
    "\n",
    "data, summary, proposed_stratifier = None, None, None\n",
    "\n",
    "country_code_map = {\n",
    "    'bangladesh': 'BGD',\n",
    "    'benin': 'BEN',\n",
    "    'burkina_faso': 'BFA',\n",
    "    'colombia': 'COL',\n",
    "    'cote_divoire': 'CIV',\n",
    "    'ethiopia': 'ETH',\n",
    "    'ghana': 'GHA',\n",
    "    'ghana_henry': 'GHA',\n",
    "    'guatemala': 'GTM',\n",
    "    'guinea-bissau': 'GNB',\n",
    "    'india': 'IND',\n",
    "    'indonesia': 'IDN',\n",
    "    'kenya': 'KEN',\n",
    "    'madagascar': 'MDG',\n",
    "    'malawi': 'MWI',\n",
    "    'mali': 'MLI',\n",
    "    'niger': 'NER',\n",
    "    'nigeria': 'NGA',\n",
    "    'rwanda': 'RWA',\n",
    "    'senegal': 'SEN',\n",
    "    'somalia': 'SOM',\n",
    "    'south_africa': 'ZAF',\n",
    "    'south_sudan': 'SSD',\n",
    "    'tanzania': 'TZA',\n",
    "    'timor-leste': 'TLS',\n",
    "    'togo': 'TGO',\n",
    "    'togo_survey_and_cdr': 'TGO',\n",
    "    'uganda': 'UGA'\n",
    "}    \n",
    "country_stratifier_map = {\n",
    "    'bangladesh': 'DivCode',\n",
    "    'benin': 'region', \n",
    "    'burkina_faso': 'region', \n",
    "    'colombia': 'domain', \n",
    "    'cote_divoire': 'region',\n",
    "    'ethiopia': 'region', #\n",
    "    'ghana': 'region', \n",
    "    'ghana_henry': 'region',\n",
    "    'guatemala': 'region',\n",
    "    'guinea-bissau': 'region',\n",
    "    'india': 'state', \n",
    "    'indonesia': 'regency_city_code',\n",
    "    'kenya': 'county', \n",
    "    'madagascar': 'region',\n",
    "    'malawi': 'ea_id', \n",
    "    'mali': 'region', \n",
    "    'niger': 's00q01',\n",
    "    'nigeria': 'ea_id', \n",
    "    'rwanda': 'district',\n",
    "    'senegal': 'region',\n",
    "    'somalia': 'region',\n",
    "    'south_africa': 'province',\n",
    "    'south_sudan': 'ea',\n",
    "    'tanzania': 'region',\n",
    "    'timor-leste': None,\n",
    "    'togo': 'cluster_id',\n",
    "    'togo_survey_and_cdr': 'cluster_id',\n",
    "    'uganda': 'region'\n",
    "}\n",
    "\n",
    "\n",
    "# for the compiled data: convert countries whose directory names don't match their lower-case country name.\n",
    "country_name_map = {\n",
    "    'burkina_faso': 'burkina faso',\n",
    "    'cote_divoire': \"coÌ‚te d'ivoire\",\n",
    "    'south_africa': 'south africa',\n",
    "    'south_sudan': 'south sudan'\n",
    "}\n",
    "#########################\n",
    "country = 'india' # Change this to the desired country\n",
    "#########################\n",
    "country_code = country_code_map[country]\n",
    "country_data_path = data_path / country_code / 'cleaned'\n",
    "\n",
    "data = pd.read_parquet(country_data_path / 'full.parquet')\n",
    "summary = pd.read_parquet(country_data_path / 'summary.parquet')\n",
    "proposed_stratifier = country_stratifier_map[country]\n",
    "\n",
    "# Read in the most recent auxiliary data file available\n",
    "aux_files = glob('/data/eop/compiled_country_data/auxiliary_data/auxiliary_data_*.csv')\n",
    "latest_file = max(aux_files, key=lambda x: x.split('_')[-1].split('.')[0])\n",
    "\n",
    "if country == 'somalia':\n",
    "    print('Warning: Sidestepping conversion doc for somalia')\n",
    "\n",
    "else:     \n",
    "    compiled_data = pd.read_csv(latest_file)\n",
    "    wb_poverty_rate_survey_year_2017 = compiled_data[\n",
    "        compiled_data.country_code == country_code\n",
    "    ].wb_poverty_rate_povertyline_2017_survey_year.values[0]\n",
    "    wb_poverty_rate_survey_year_2021 = compiled_data[\n",
    "        compiled_data.country_code == country_code\n",
    "    ].wb_poverty_rate_povertyline_2021_survey_year.values[0]\n",
    "    conversion_2021_to_2017 = compiled_data[\n",
    "        compiled_data.country_code == country_code\n",
    "    ].overall_conversion_factor_ratio_from_2021_to_2017.values[0]\n",
    "\n",
    "if 'variable_description' not in summary.columns:\n",
    "    summary['variable_description'] = summary['variable_name']\n",
    "\n",
    "print(f'Read in: {country}')\n",
    "print('nullity: ')\n",
    "display(data.isna().mean().sort_values(ascending=False).head(2))\n",
    "# Empty string may or may not be a problem.\n",
    "\n",
    "print('empty string')\n",
    "display(data.isin(['']).mean().sort_values(ascending=False).head(2))\n",
    "print('Number of samples:')\n",
    "print(data.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary correctness: Matches data, format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "country: india\n",
      "Variables in summary but not in data: set()\n",
      "Columns in data but not in summary: set()\n"
     ]
    }
   ],
   "source": [
    "print(f'country: {country}')\n",
    "\n",
    "# check that metadata and data match\n",
    "data_columns = set(data.columns)\n",
    "\n",
    "summary_variable_names = set(summary['variable_name'])\n",
    "missing_in_data = summary_variable_names - data_columns\n",
    "missing_in_summary = data_columns - summary_variable_names\n",
    "\n",
    "print(\"Variables in summary but not in data:\", missing_in_data)\n",
    "print(\"Columns in data but not in summary:\", missing_in_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Missingness columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "country: india\n",
      "categorical columns with missingness indicators:\n",
      "Series([], Name: count, dtype: int64)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>variable_name</th>\n",
       "      <th>data_type</th>\n",
       "      <th>geographic_indicator</th>\n",
       "      <th>geographic_indicator_coarser</th>\n",
       "      <th>variable_description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [variable_name, data_type, geographic_indicator, geographic_indicator_coarser, variable_description]\n",
       "Index: []"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numerical columns with no missingness indicators:\n",
      "5                             hh_wgt\n",
      "6                           head_age\n",
      "10                           hh_size\n",
      "11                          num_male\n",
      "12                        num_female\n",
      "13                         num_child\n",
      "14                         num_adult\n",
      "15                       num_elderly\n",
      "19                  area_owned_acres\n",
      "42    consumption_per_capita_per_day\n",
      "43         headcount_adjusted_hh_wgt\n",
      "Name: variable_name, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Missingness columns (assumes _missing suffix)\n",
    "print(f'country: {country}')\n",
    "\n",
    "missingness_columns_missing = [\n",
    "    c for c in data.columns if ('missing' in c) \n",
    "]\n",
    "missingness_columns_m = [\n",
    "    c for c in data.columns if ('_m' in c) \n",
    "]\n",
    "with_missingness = [\n",
    "    c[:-8] for c in missingness_columns_missing\n",
    "] + [\n",
    "    c[:-2] for c in missingness_columns_m\n",
    "]\n",
    "missingness_columns = missingness_columns_missing + missingness_columns_m\n",
    "for c in missingness_columns:\n",
    "    if not (c in summary.variable_name.values):\n",
    "        print(f\"Missingness column {c} not in summary\")\n",
    "    \n",
    "relevant_summary = summary[summary.variable_name.isin(with_missingness)]\n",
    "print('categorical columns with missingness indicators:')\n",
    "\n",
    "print(relevant_summary.data_type.value_counts())\n",
    "display(relevant_summary[relevant_summary.data_type == 'categorical'])\n",
    "\n",
    "# print numerical columns with no missingness indicators\n",
    "print('numerical columns with no missingness indicators:')\n",
    "print(summary[\n",
    "    (summary.data_type == 'numeric') \n",
    "    & (~summary.variable_name.isin(with_missingness))\n",
    "    & (~summary.variable_name.str.endswith('_missing'))\n",
    "    & (~summary.variable_name.str.endswith('_m'))\n",
    "].variable_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Consumption, weights, hh size, poverty rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "country: india\n",
      "mean: 8.803151\n",
      "std: 7.090850830078125\n",
      "survey rate (2017 PPP, 2.15 line): 0.03343365990658686\n",
      "wb rate (2017 PPP, 2.15 line): 0.023491840878\n",
      "discrepancy: 0.00994181902858686\n",
      "survey rate (2021 PPP, 3.00 line): 0.06564702310030764\n",
      "wb rate (2021 PPP, 3.00 line): 0.052516401\n",
      "discrepancy: 0.013130622100307639\n"
     ]
    }
   ],
   "source": [
    "print(f'country: {country}')\n",
    "assert 'consumption_per_capita_per_day' in data.columns\n",
    "if 'hhid' in data.columns:\n",
    "    assert data.hhid.is_unique, \"'hhid' is not unique\"\n",
    "assert 'headcount_adjusted_hh_wgt' in data.columns\n",
    "assert pd.api.types.is_numeric_dtype(data['consumption_per_capita_per_day']), \"'consumption_per_capita_per_day' is not numeric\"\n",
    "assert pd.api.types.is_numeric_dtype(data['headcount_adjusted_hh_wgt']), \"'headcount_adjusted_hh_wgt' is not numeric\"\n",
    "if not 'hh_size' in data.columns:\n",
    "    print('Warning: Missing hh_size')\n",
    "else:\n",
    "    if not np.isclose(data.hh_size * data.hh_wgt, data.headcount_adjusted_hh_wgt).all():\n",
    "        print('Warning: hh_size * hh_wgt does not equal headcount_adjusted_hh_wgt for all rows')\n",
    "    assert pd.api.types.is_numeric_dtype(data['hh_size']), \"'hh_size' is not numeric\"\n",
    "\n",
    "for col in ['headcount_adjusted_hh_wgt_missing', 'consumption_per_capita_per_day_missing', 'hh_wgt_missing']:\n",
    "    if col in data.columns:\n",
    "        assert data[col].sum() == 0, f\"{col} has missing values\"\n",
    "\n",
    "\n",
    "print('mean:', data.consumption_per_capita_per_day.mean())\n",
    "print('std:', data.consumption_per_capita_per_day.std())\n",
    "\n",
    "consumption_adjusted = data.consumption_per_capita_per_day * conversion_2021_to_2017\n",
    "\n",
    "count_poor = (\n",
    "    data[consumption_adjusted < 2.15].headcount_adjusted_hh_wgt\n",
    ").sum()\n",
    "\n",
    "total = (\n",
    "    data.headcount_adjusted_hh_wgt\n",
    ").sum()\n",
    "rate = count_poor / total\n",
    "\n",
    "print('survey rate (2017 PPP, 2.15 line):',rate)\n",
    "print('wb rate (2017 PPP, 2.15 line):', wb_poverty_rate_survey_year_2017)\n",
    "print('discrepancy:', rate - wb_poverty_rate_survey_year_2017)\n",
    "\n",
    "count_poor = (\n",
    "    data[data.consumption_per_capita_per_day < 3].headcount_adjusted_hh_wgt\n",
    ").sum()\n",
    "\n",
    "total = (\n",
    "    data.headcount_adjusted_hh_wgt\n",
    ").sum()\n",
    "rate = count_poor / total\n",
    "\n",
    "print('survey rate (2021 PPP, 3.00 line):',rate)\n",
    "print('wb rate (2021 PPP, 3.00 line):', wb_poverty_rate_survey_year_2021)\n",
    "print('discrepancy:', rate - wb_poverty_rate_survey_year_2021)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Suspiciously named columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "country: india\n",
      "containing the word \"unit\":\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>variable_name</th>\n",
       "      <th>data_type</th>\n",
       "      <th>geographic_indicator</th>\n",
       "      <th>geographic_indicator_coarser</th>\n",
       "      <th>variable_description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>type_dwelling_unit</td>\n",
       "      <td>categorical</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>type_dwelling_unit</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         variable_name    data_type  geographic_indicator  \\\n",
       "20  type_dwelling_unit  categorical                 False   \n",
       "\n",
       "    geographic_indicator_coarser variable_description  \n",
       "20                         False   type_dwelling_unit  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "containing the word \"consumption\":\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>variable_name</th>\n",
       "      <th>data_type</th>\n",
       "      <th>geographic_indicator</th>\n",
       "      <th>geographic_indicator_coarser</th>\n",
       "      <th>variable_description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>consumption_per_capita_per_day</td>\n",
       "      <td>numeric</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>consumption_per_capita_per_day</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     variable_name data_type  geographic_indicator  \\\n",
       "42  consumption_per_capita_per_day   numeric                 False   \n",
       "\n",
       "    geographic_indicator_coarser            variable_description  \n",
       "42                         False  consumption_per_capita_per_day  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "containing suspicious demographic words\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>variable_name</th>\n",
       "      <th>data_type</th>\n",
       "      <th>geographic_indicator</th>\n",
       "      <th>geographic_indicator_coarser</th>\n",
       "      <th>variable_description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [variable_name, data_type, geographic_indicator, geographic_indicator_coarser, variable_description]\n",
       "Index: []"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "variables with \"id\" or \"code\" and listed numeric:\n"
     ]
    }
   ],
   "source": [
    "print(f'country: {country}')\n",
    "\n",
    "# Suspicious data\n",
    "print('containing the word \"unit\":')\n",
    "display(\n",
    "    summary[\n",
    "        (\n",
    "            summary.variable_name.str.contains('unit')\n",
    "            | summary.variable_description.str.contains('unit')\n",
    "        ) & (\n",
    "            ~summary.variable_name.str.contains('community')\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "\n",
    "print('containing the word \"consumption\":')\n",
    "display(\n",
    "    summary[\n",
    "        summary.variable_name.str.contains('consumption')\n",
    "        | summary.variable_description.str.contains('consumption')\n",
    "    ]\n",
    ")\n",
    "\n",
    "print('containing suspicious demographic words')\n",
    "display(\n",
    "    summary[\n",
    "        summary.variable_name.str.contains('relig|ethn|nationality')\n",
    "        | summary.variable_description.str.contains('relig|ethn|nationality')\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Print variables whose name contains \"id\" or \"code\" and are listed as numeric in the summary\n",
    "print('variables with \"id\" or \"code\" and listed numeric:')\n",
    "\n",
    "filtered_variables = summary[\n",
    "    (summary[\"variable_name\"].str.contains(\"id|code\", case=False, na=False)) &\n",
    "    (summary[\"data_type\"] == \"numeric\")\n",
    "]\n",
    "\n",
    "# Print the name and description of the filtered variables\n",
    "for _, row in filtered_variables.iterrows():\n",
    "    print(f\"Name: {row['variable_name']}, Description: {row['variable_description']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "country: india\n"
     ]
    }
   ],
   "source": [
    "print(f'country: {country}')\n",
    "# Check that \"summary\" fits the required format\n",
    "required_columns = {\n",
    "    \"variable_name\", \"data_type\", \"geographic_indicator\", \"geographic_indicator_coarser\"\n",
    "    }\n",
    "summary_columns = set(summary.columns)\n",
    "\n",
    "missing_columns = required_columns - summary_columns\n",
    "if missing_columns:\n",
    "    raise ValueError(f\"Missing required columns in summary: {missing_columns}\")\n",
    "\n",
    "# Ensure \"data_type\" has only permitted values\n",
    "permitted_data_types = {\"numeric\", \"categorical\"}\n",
    "found_errors = False\n",
    "for _, row in summary.iterrows():\n",
    "    if row[\"data_type\"] not in permitted_data_types:\n",
    "        print(\n",
    "            f\"Invalid data_type '{row['data_type']}' for variable '{row['variable_name']}'. \"\n",
    "            f\"Description: {row['variable_description']}\"\n",
    "        )\n",
    "        found_errors = True\n",
    "\n",
    "# Ensure \"geographic_indicator_coarser\", \"geographic_indicator_finer\" is boolean or 0-1\n",
    "for _, row in summary.iterrows():\n",
    "    for c in [\"geographic_indicator\", \"geographic_indicator_coarser\", \"geographic_indicator_finer\"]:\n",
    "        if c not in row:\n",
    "            continue\n",
    "        if row[c] not in [0, 1, True, False, None]:\n",
    "            print(\n",
    "                f\"Invalid {c} '{row[c]}' for variable '{row['variable_name']}'. \"\n",
    "                f\"Description: {row['variable_description']}\"\n",
    "            )\n",
    "            found_errors = True\n",
    "if found_errors:\n",
    "    raise ValueError(\"Errors found in summary metadata. Please fix them before proceeding.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "country: india\n"
     ]
    }
   ],
   "source": [
    "print(f'country: {country}')\n",
    "\n",
    "# Check that numeric columns in summary are actually numeric in data\n",
    "numeric_columns = summary[summary[\"data_type\"] == \"numeric\"][\"variable_name\"]\n",
    "found_error = False\n",
    "for col in numeric_columns:\n",
    "    if col in data.columns and (not pd.api.types.is_numeric_dtype(data[col])):\n",
    "        description = summary.loc[summary[\"variable_name\"] == col, \"variable_description\"].values[0]\n",
    "        print(f\"BAD: numeric in summary, non-numeric in data: '{col}'; {description}\")\n",
    "        found_error = True\n",
    "if found_error:\n",
    "    raise ValueError('Found numeric columns in summary that are not numeric in data.')\n",
    "\n",
    "# Check that categorical columns in summary are actually categorical in data (less important)\n",
    "categorical_columns = summary[summary[\"data_type\"] == \"categorical\"][\"variable_name\"]\n",
    "for col in categorical_columns:\n",
    "    if (\n",
    "        col in data.columns \n",
    "        and pd.api.types.is_numeric_dtype(data[col])\n",
    "        and not (col.endswith('_missing') or col.endswith('_m'))\n",
    "    ):\n",
    "        description = summary.loc[summary[\"variable_name\"] == col, \"variable_description\"].values[0]\n",
    "        print(f\"categorical in summary, numeric in data: '{col}'; {description}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Duplicate columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "country: india\n",
      "Warning: Skipping equivalent-column check\n"
     ]
    }
   ],
   "source": [
    "# Check for duplicate information. Don't do if there are too many columns (i.e. remote sensing/CDR).\n",
    "print(f'country: {country}')\n",
    "\n",
    "if False:\n",
    "    find_equivalent_columns(data, summary)\n",
    "else:\n",
    "    print('Warning: Skipping equivalent-column check')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Geography and stratification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "country: india\n",
      "geographic indicators:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>variable_name</th>\n",
       "      <th>data_type</th>\n",
       "      <th>geographic_indicator</th>\n",
       "      <th>geographic_indicator_coarser</th>\n",
       "      <th>variable_description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>state</td>\n",
       "      <td>categorical</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>state</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>nss_region</td>\n",
       "      <td>categorical</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>nss_region</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>district</td>\n",
       "      <td>categorical</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>district</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  variable_name    data_type  geographic_indicator  \\\n",
       "2         state  categorical                  True   \n",
       "3    nss_region  categorical                  True   \n",
       "4      district  categorical                  True   \n",
       "\n",
       "   geographic_indicator_coarser variable_description  \n",
       "2                          True                state  \n",
       "3                         False           nss_region  \n",
       "4                         False             district  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state\n",
      "36\n",
      "nss_region\n",
      "87\n",
      "district\n",
      "71\n"
     ]
    }
   ],
   "source": [
    "print(f'country: {country}')\n",
    "\n",
    "print('geographic indicators:')\n",
    "display(summary[summary.geographic_indicator])\n",
    "for _, row in summary[summary.geographic_indicator].iterrows():\n",
    "    print(row.variable_name)\n",
    "    print(data[row.variable_name].nunique())\n",
    "\n",
    "if False:  # Holdover from previous partially-represented step\n",
    "    partially_represented = summary[\n",
    "        (summary.geographic_indicator_finer) & ~(summary.geographic_indicator_coarser)\n",
    "    ]\n",
    "    if len(partially_represented) == 0:\n",
    "        print('No partially represented geo level')\n",
    "    else:\n",
    "        assert len(partially_represented) == 1\n",
    "        print('partially represented:')\n",
    "        print(partially_represented.variable_name.values[0])\n",
    "        print('partially represented value counts:')\n",
    "        print(data[partially_represented.variable_name.values[0]].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "country: india\n",
      "proposed stratifier: state\n",
      "count per unit\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>state</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>09. Uttar Pradesh</td>\n",
       "      <td>30239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>27. Maharashtra</td>\n",
       "      <td>22759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>19. West Bengal</td>\n",
       "      <td>18136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10. Bihar</td>\n",
       "      <td>17184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>33. Tamil Nadu</td>\n",
       "      <td>14364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>23. Madhya Pradesh</td>\n",
       "      <td>14197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>08. Rajasthan</td>\n",
       "      <td>13162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>29. Karnataka</td>\n",
       "      <td>12389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>24. Gujarat</td>\n",
       "      <td>11286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>28. Andhra Pradesh</td>\n",
       "      <td>10283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>21. Odisha</td>\n",
       "      <td>9185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>18. Assam</td>\n",
       "      <td>8562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>32. Kerala</td>\n",
       "      <td>7377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>36. Telangana</td>\n",
       "      <td>6786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>20. Jharkhand</td>\n",
       "      <td>6385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>03. Punjab</td>\n",
       "      <td>5830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>06. Haryana</td>\n",
       "      <td>5268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>16. Tripura</td>\n",
       "      <td>5022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>22. Chhattisgarh</td>\n",
       "      <td>5008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>14. Manipur</td>\n",
       "      <td>4833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>12. Arunachal Pradesh</td>\n",
       "      <td>4021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>15. Mizoram</td>\n",
       "      <td>3596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>01. Jammu &amp; Kashmir</td>\n",
       "      <td>3533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>07. Delhi</td>\n",
       "      <td>3236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>17. Meghalaya</td>\n",
       "      <td>3211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>13. Nagaland</td>\n",
       "      <td>3075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>05. Uttarakhand</td>\n",
       "      <td>2773</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>02. Himachal Pradesh</td>\n",
       "      <td>2442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>11. Sikkim</td>\n",
       "      <td>2131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>34. Puducherry (U.T.)</td>\n",
       "      <td>1070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>35. Andaman &amp; Nicobar Islands (U.T.)</td>\n",
       "      <td>1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>04. Chandigarh (U.T.)</td>\n",
       "      <td>720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>37. Ladakh (U.T.)</td>\n",
       "      <td>719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>30. Goa</td>\n",
       "      <td>683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>25. Dadra &amp; Nagar Haveli and Daman &amp; Diu</td>\n",
       "      <td>674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>31. Lakshadweep (U.T.)</td>\n",
       "      <td>607</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       state  count\n",
       "8                          09. Uttar Pradesh  30239\n",
       "25                           27. Maharashtra  22759\n",
       "18                           19. West Bengal  18136\n",
       "9                                  10. Bihar  17184\n",
       "31                            33. Tamil Nadu  14364\n",
       "22                        23. Madhya Pradesh  14197\n",
       "7                              08. Rajasthan  13162\n",
       "27                             29. Karnataka  12389\n",
       "23                               24. Gujarat  11286\n",
       "26                        28. Andhra Pradesh  10283\n",
       "20                                21. Odisha   9185\n",
       "17                                 18. Assam   8562\n",
       "30                                32. Kerala   7377\n",
       "34                             36. Telangana   6786\n",
       "19                             20. Jharkhand   6385\n",
       "2                                 03. Punjab   5830\n",
       "5                                06. Haryana   5268\n",
       "15                               16. Tripura   5022\n",
       "21                          22. Chhattisgarh   5008\n",
       "13                               14. Manipur   4833\n",
       "11                     12. Arunachal Pradesh   4021\n",
       "14                               15. Mizoram   3596\n",
       "0                        01. Jammu & Kashmir   3533\n",
       "6                                  07. Delhi   3236\n",
       "16                             17. Meghalaya   3211\n",
       "12                              13. Nagaland   3075\n",
       "4                            05. Uttarakhand   2773\n",
       "1                       02. Himachal Pradesh   2442\n",
       "10                                11. Sikkim   2131\n",
       "32                     34. Puducherry (U.T.)   1070\n",
       "33      35. Andaman & Nicobar Islands (U.T.)   1000\n",
       "3                      04. Chandigarh (U.T.)    720\n",
       "35                         37. Ladakh (U.T.)    719\n",
       "28                                   30. Goa    683\n",
       "24  25. Dadra & Nagar Haveli and Daman & Diu    674\n",
       "29                    31. Lakshadweep (U.T.)    607"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weights per unit\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "state\n",
       "31. Lakshadweep (U.T.)                        36\n",
       "25. Dadra & Nagar Haveli and Daman & Diu      51\n",
       "37. Ladakh (U.T.)                             58\n",
       "30. Goa                                       72\n",
       "04. Chandigarh (U.T.)                         77\n",
       "35. Andaman & Nicobar Islands (U.T.)         104\n",
       "34. Puducherry (U.T.)                        112\n",
       "11. Sikkim                                   202\n",
       "17. Meghalaya                                250\n",
       "05. Uttarakhand                              258\n",
       "02. Himachal Pradesh                         265\n",
       "15. Mizoram                                  337\n",
       "07. Delhi                                    338\n",
       "01. Jammu & Kashmir                          349\n",
       "13. Nagaland                                 384\n",
       "12. Arunachal Pradesh                        412\n",
       "16. Tripura                                  430\n",
       "14. Manipur                                  442\n",
       "22. Chhattisgarh                             507\n",
       "06. Haryana                                  526\n",
       "20. Jharkhand                                604\n",
       "03. Punjab                                   642\n",
       "36. Telangana                                663\n",
       "18. Assam                                    740\n",
       "32. Kerala                                   793\n",
       "21. Odisha                                   886\n",
       "28. Andhra Pradesh                           989\n",
       "24. Gujarat                                 1128\n",
       "29. Karnataka                               1285\n",
       "08. Rajasthan                               1367\n",
       "23. Madhya Pradesh                          1417\n",
       "33. Tamil Nadu                              1423\n",
       "10. Bihar                                   1510\n",
       "19. West Bengal                             1609\n",
       "27. Maharashtra                             2351\n",
       "09. Uttar Pradesh                           2963\n",
       "Name: hh_wgt, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "regions per weight class\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "hh_wgt\n",
       "472       1\n",
       "142439    1\n",
       "142428    1\n",
       "142411    1\n",
       "142402    1\n",
       "         ..\n",
       "136500    5\n",
       "7920      5\n",
       "117000    5\n",
       "81900     5\n",
       "161700    5\n",
       "Name: state, Length: 23607, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count per unit x weight\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hh_wgt</th>\n",
       "      <th>state</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2423</th>\n",
       "      <td>11830</td>\n",
       "      <td>12. Arunachal Pradesh</td>\n",
       "      <td>165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>1130</td>\n",
       "      <td>12. Arunachal Pradesh</td>\n",
       "      <td>90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>716</th>\n",
       "      <td>4805</td>\n",
       "      <td>12. Arunachal Pradesh</td>\n",
       "      <td>73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6056</th>\n",
       "      <td>52880</td>\n",
       "      <td>18. Assam</td>\n",
       "      <td>71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5438</th>\n",
       "      <td>44370</td>\n",
       "      <td>09. Uttar Pradesh</td>\n",
       "      <td>69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22737</th>\n",
       "      <td>200994</td>\n",
       "      <td>08. Rajasthan</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22739</th>\n",
       "      <td>201041</td>\n",
       "      <td>19. West Bengal</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22742</th>\n",
       "      <td>201094</td>\n",
       "      <td>08. Rajasthan</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22744</th>\n",
       "      <td>201106</td>\n",
       "      <td>09. Uttar Pradesh</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8324</th>\n",
       "      <td>79900</td>\n",
       "      <td>24. Gujarat</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>25580 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       hh_wgt                  state  count\n",
       "2423    11830  12. Arunachal Pradesh    165\n",
       "23       1130  12. Arunachal Pradesh     90\n",
       "716      4805  12. Arunachal Pradesh     73\n",
       "6056    52880              18. Assam     71\n",
       "5438    44370      09. Uttar Pradesh     69\n",
       "...       ...                    ...    ...\n",
       "22737  200994          08. Rajasthan      1\n",
       "22739  201041        19. West Bengal      1\n",
       "22742  201094          08. Rajasthan      1\n",
       "22744  201106      09. Uttar Pradesh      1\n",
       "8324    79900            24. Gujarat      1\n",
       "\n",
       "[25580 rows x 3 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Stratification\n",
    "print(f'country: {country}')\n",
    "\n",
    "if country == 'colombia':\n",
    "    data['strat'] = data.domain.astype(str) + '_' + data.region.astype(str)\n",
    "    proposed_stratifier = 'strat'\n",
    "    print('Colombia: Creating stratifier \"strat\" = domain + region')\n",
    "\n",
    "print('proposed stratifier:', proposed_stratifier)\n",
    "print('count per unit')\n",
    "display(data.groupby(proposed_stratifier, observed=True).size().reset_index(name='count').sort_values('count', ascending=False))\n",
    "\n",
    "print('weights per unit')\n",
    "display(data.groupby(proposed_stratifier,  observed=True).hh_wgt.nunique().sort_values())\n",
    "\n",
    "print('regions per weight class')\n",
    "display(data.groupby('hh_wgt', observed=True)[proposed_stratifier].nunique().sort_values())\n",
    "\n",
    "print('count per unit x weight')\n",
    "display(\n",
    "    data.groupby(['hh_wgt', proposed_stratifier],  observed=True)\n",
    "    .size()\n",
    "    .reset_index(name='count')\n",
    "    .sort_values('count', ascending=False)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Look at the columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "country: india\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>variable_name</th>\n",
       "      <th>data_type</th>\n",
       "      <th>geographic_indicator</th>\n",
       "      <th>geographic_indicator_coarser</th>\n",
       "      <th>variable_description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>hhid</td>\n",
       "      <td>categorical</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>hhid</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sector</td>\n",
       "      <td>categorical</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>sector</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>state</td>\n",
       "      <td>categorical</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>state</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>nss_region</td>\n",
       "      <td>categorical</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>nss_region</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>district</td>\n",
       "      <td>categorical</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>district</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>hh_wgt</td>\n",
       "      <td>numeric</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>hh_wgt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>head_age</td>\n",
       "      <td>numeric</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>head_age</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>head_gender</td>\n",
       "      <td>categorical</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>head_gender</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>head_education</td>\n",
       "      <td>categorical</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>head_education</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>hh_size</td>\n",
       "      <td>numeric</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>hh_size</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>num_female</td>\n",
       "      <td>numeric</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>num_female</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>num_child</td>\n",
       "      <td>numeric</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>num_child</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>num_adult</td>\n",
       "      <td>numeric</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>num_adult</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>num_elderly</td>\n",
       "      <td>numeric</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>num_elderly</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>max_adult_education</td>\n",
       "      <td>categorical</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>max_adult_education</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>max_female_education</td>\n",
       "      <td>categorical</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>max_female_education</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>land_owned</td>\n",
       "      <td>categorical</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>land_owned</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>area_owned_acres</td>\n",
       "      <td>numeric</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>area_owned_acres</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>type_dwelling_unit</td>\n",
       "      <td>categorical</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>type_dwelling_unit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>energy_cooking</td>\n",
       "      <td>categorical</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>energy_cooking</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>energy_lighting</td>\n",
       "      <td>categorical</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>energy_lighting</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>water_source</td>\n",
       "      <td>categorical</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>water_source</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>latrine_access_type</td>\n",
       "      <td>categorical</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>latrine_access_type</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>latrine_type</td>\n",
       "      <td>categorical</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>latrine_type</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>ration_card_type</td>\n",
       "      <td>categorical</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>ration_card_type</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>access_television</td>\n",
       "      <td>categorical</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>access_television</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>access_radio</td>\n",
       "      <td>categorical</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>access_radio</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>access_laptop_pc</td>\n",
       "      <td>categorical</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>access_laptop_pc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>access_bicycle</td>\n",
       "      <td>categorical</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>access_bicycle</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>access_car_jeep_van</td>\n",
       "      <td>categorical</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>access_car_jeep_van</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>access_truck</td>\n",
       "      <td>categorical</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>access_truck</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>access_animal_cart</td>\n",
       "      <td>categorical</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>access_animal_cart</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>access_refrigerator</td>\n",
       "      <td>categorical</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>access_refrigerator</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>access_air_conditioner</td>\n",
       "      <td>categorical</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>access_air_conditioner</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>consumption_per_capita_per_day</td>\n",
       "      <td>numeric</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>consumption_per_capita_per_day</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>headcount_adjusted_hh_wgt</td>\n",
       "      <td>numeric</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>headcount_adjusted_hh_wgt</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     variable_name    data_type  geographic_indicator  \\\n",
       "0                             hhid  categorical                 False   \n",
       "1                           sector  categorical                 False   \n",
       "2                            state  categorical                  True   \n",
       "3                       nss_region  categorical                  True   \n",
       "4                         district  categorical                  True   \n",
       "5                           hh_wgt      numeric                 False   \n",
       "6                         head_age      numeric                 False   \n",
       "7                      head_gender  categorical                 False   \n",
       "9                   head_education  categorical                 False   \n",
       "10                         hh_size      numeric                 False   \n",
       "12                      num_female      numeric                 False   \n",
       "13                       num_child      numeric                 False   \n",
       "14                       num_adult      numeric                 False   \n",
       "15                     num_elderly      numeric                 False   \n",
       "16             max_adult_education  categorical                 False   \n",
       "17            max_female_education  categorical                 False   \n",
       "18                      land_owned  categorical                 False   \n",
       "19                area_owned_acres      numeric                 False   \n",
       "20              type_dwelling_unit  categorical                 False   \n",
       "24                  energy_cooking  categorical                 False   \n",
       "25                 energy_lighting  categorical                 False   \n",
       "26                    water_source  categorical                 False   \n",
       "27             latrine_access_type  categorical                 False   \n",
       "28                    latrine_type  categorical                 False   \n",
       "29                ration_card_type  categorical                 False   \n",
       "30               access_television  categorical                 False   \n",
       "31                    access_radio  categorical                 False   \n",
       "32                access_laptop_pc  categorical                 False   \n",
       "34                  access_bicycle  categorical                 False   \n",
       "36             access_car_jeep_van  categorical                 False   \n",
       "37                    access_truck  categorical                 False   \n",
       "38              access_animal_cart  categorical                 False   \n",
       "39             access_refrigerator  categorical                 False   \n",
       "41          access_air_conditioner  categorical                 False   \n",
       "42  consumption_per_capita_per_day      numeric                 False   \n",
       "43       headcount_adjusted_hh_wgt      numeric                 False   \n",
       "\n",
       "    geographic_indicator_coarser            variable_description  \n",
       "0                          False                            hhid  \n",
       "1                          False                          sector  \n",
       "2                           True                           state  \n",
       "3                          False                      nss_region  \n",
       "4                          False                        district  \n",
       "5                          False                          hh_wgt  \n",
       "6                          False                        head_age  \n",
       "7                          False                     head_gender  \n",
       "9                          False                  head_education  \n",
       "10                         False                         hh_size  \n",
       "12                         False                      num_female  \n",
       "13                         False                       num_child  \n",
       "14                         False                       num_adult  \n",
       "15                         False                     num_elderly  \n",
       "16                         False             max_adult_education  \n",
       "17                         False            max_female_education  \n",
       "18                         False                      land_owned  \n",
       "19                         False                area_owned_acres  \n",
       "20                         False              type_dwelling_unit  \n",
       "24                         False                  energy_cooking  \n",
       "25                         False                 energy_lighting  \n",
       "26                         False                    water_source  \n",
       "27                         False             latrine_access_type  \n",
       "28                         False                    latrine_type  \n",
       "29                         False                ration_card_type  \n",
       "30                         False               access_television  \n",
       "31                         False                    access_radio  \n",
       "32                         False                access_laptop_pc  \n",
       "34                         False                  access_bicycle  \n",
       "36                         False             access_car_jeep_van  \n",
       "37                         False                    access_truck  \n",
       "38                         False              access_animal_cart  \n",
       "39                         False             access_refrigerator  \n",
       "41                         False          access_air_conditioner  \n",
       "42                         False  consumption_per_capita_per_day  \n",
       "43                         False       headcount_adjusted_hh_wgt  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(f'country: {country}')\n",
    "if True:\n",
    "    with pd.option_context('display.max_rows', 300):\n",
    "        display(summary[~summary.variable_name.str.contains('_m')])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split data, check with roshni's code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Country: india, country data path: /data/eop/country_data/IND/cleaned\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CompletedProcess(args=['bash', '/data/eop/country_data/IND/cleaned/split.sh'], returncode=0)"
      ]
     },
     "execution_count": 315,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(f'Country: {country}, country data path: {country_data_path}')\n",
    "os.chdir(country_data_path)\n",
    "subprocess.run(['bash', str(country_data_path / 'split.sh')], check=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Country: india, country data path: /data/eop/country_data/IND/cleaned\n",
      "Top 5 categorical columns by distinct values:\n",
      "state: 36\n",
      "water_source: 17\n",
      "max_adult_education: 13\n",
      "max_female_education: 13\n",
      "energy_cooking: 12\n",
      "Top 5 categorical columns by distinct values:\n",
      "state: 36\n",
      "water_source: 17\n",
      "max_adult_education: 13\n",
      "max_female_education: 13\n",
      "energy_cooking: 12\n",
      "Top 5 categorical columns by distinct values:\n",
      "state: 36\n",
      "water_source: 17\n",
      "max_adult_education: 13\n",
      "max_female_education: 13\n",
      "energy_cooking: 12\n",
      "In-sample weighted R^2: 0.4545\n"
     ]
    }
   ],
   "source": [
    "print(f'Country: {country}, country data path: {country_data_path}')\n",
    "\n",
    "train_path = country_data_path / 'train.parquet'\n",
    "test_path = country_data_path / 'test.parquet'\n",
    "full_data_path = country_data_path / 'full.parquet'\n",
    "assert len(pd.read_parquet(train_path)) + len(pd.read_parquet(test_path)) == len(pd.read_parquet(full_data_path))\n",
    "summary_path = country_data_path / 'summary.parquet'\n",
    "# Assert that train.parquet and test.parquet are newer than any other parquet file under country_data_path\n",
    "all_parquet_files = list(country_data_path.glob('*.parquet'))\n",
    "for f in all_parquet_files:\n",
    "    if f not in [train_path, test_path]:\n",
    "        assert train_path.stat().st_mtime > f.stat().st_mtime, f\"{train_path.name} is not newer than {f.name}\"\n",
    "        assert test_path.stat().st_mtime > f.stat().st_mtime, f\"{test_path.name} is not newer than {f.name}\"\n",
    "\n",
    "train_dataset, validation_dataset, test_covariate_dataset, test_dataset = load_datasets(\n",
    "    trainpath = train_path,\n",
    "    testpath = test_path,\n",
    "    summarypath = summary_path,\n",
    "    geo_extrapolation = True,\n",
    "    outcome = 'consumption_per_capita_per_day',\n",
    "    weight = 'headcount_adjusted_hh_wgt'\n",
    ")\n",
    "X, y, r = train_dataset.get_data()\n",
    "X, X_mean, X_std = standardize(X)\n",
    "y, y_mean, y_std = standardize(y)\n",
    "model = LinearRegression(fit_intercept=True)\n",
    "model.fit(X, y, sample_weight=r)\n",
    "# Report in-sample R^2\n",
    "r2_weighted = model.score(X, y, sample_weight=r)\n",
    "print(f\"In-sample weighted R^2: {r2_weighted:.4f}\")\n",
    "if r2_weighted > 0.9:\n",
    "    raise AssertionError(\"In-sample weighted R^2 is suspiciously high (>0.9), please check for data leakage.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "leo_base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
